from fastapi import WebSocket, WebSocketDisconnect
import logging
import asyncio
from deepgram import DeepgramClient, LiveOptions, LiveTranscriptionEvents, SpeakOptions
from dotenv import load_dotenv
import os
import json
import queue
import tempfile
import base64
import re

from ..agent.response import ai_response

from .helper import send_heartbeat

load_dotenv()

DEEPGRAM_API_KEY = os.getenv('DEEPGRAM_API_KEY')
deepgram = DeepgramClient(DEEPGRAM_API_KEY)

async def live_audio_transcription(output_message):
    try:
        # Create a temporary file to store the audio
        with tempfile.NamedTemporaryFile(suffix='.mp3', delete=False) as temp_file:
            temp_filename = temp_file.name
            
        # Configure speech options
        speak_options = {"text": output_message}
        options = SpeakOptions(
            model="aura-2-thalia-en",
        )
        
        # Generate and save the audio
        response = deepgram.speak.rest.v("1").save(temp_filename, speak_options, options)
        
        # Read the file as bytes
        with open(temp_filename, 'rb') as f:
            audio_bytes = f.read()
            
        # Clean up the temporary file
        try:
            os.unlink(temp_filename)
        except:
            pass
            
        # Return the audio bytes
        return audio_bytes
    
    except Exception as e:
        logging.error(f"Error generating speech: {e}")
        return None

async def live_text_transcription(websocket: WebSocket):
    """This method uses deepgram api to convert audio into text, which is
    sent to the client side via a websocket. In this current version I am
    returning text, but in the next version I will be returning audio
    generated by deepgram.

    Args:
        websocket (WebSocket): This is the current open websocket, which 
        must have a connection estabilished before using deepgram to convert
        the audio to text.
    """
    
    transcript_queue = queue.Queue()
    
    async def process_transcript():
        """Process and send the output from the current transcript queue to 
        the frontend via websockets.
        """
        while True:
            if not transcript_queue.empty():
                transcript = transcript_queue.get()
                
                # Send back output to frontend using websocket
                print(f"User Input: {transcript}")
                response_text = ai_response(transcript)
                print(f"AI Response: {response_text}")
                
                audio_task = asyncio.create_task(live_audio_transcription(response_text))
                
                audio_bytes = await audio_task
                if audio_bytes:
                    audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')
                    await websocket.send_text(json.dumps({
                        "audio": audio_base64,
                        "content_type": "audio/mp3"
                    }))
                    
                await websocket.send_text(json.dumps({"transcript": response_text}))
                
            await asyncio.sleep(0.1)
    
    # Run in a seperate task
    queue_task = asyncio.create_task(process_transcript())
    
    try:
        conn = deepgram.listen.websocket.v("1")
        
        # Handle incoming transcription results with a thread-safe queue
        def on_message(sender, result, **kwargs):
            try:
                transcript = result.channel.alternatives[0].transcript
                if len(transcript) > 0:
                    logging.info(f"Transcript: {transcript}")

                    transcript_queue.put(transcript)
            except Exception as e:
                logging.error(f"Error processing transcript: {e}")
        
        # Deepgram connection
        conn.on(LiveTranscriptionEvents.Transcript, on_message)
        
        options = LiveOptions(
            model="nova-3", 
            interim_results=False, 
            language="en-US",
            punctuate=True,
            diarize=True
        )
        
        conn.start(options)
        
        # Process audio from user
        while True:
            try:
                # Send heartbeat until we recieve the bytes
                heartbeat_task = asyncio.create_task(send_heartbeat(conn))
                audio_data = await websocket.receive_bytes()                
                heartbeat_task.cancel()
                
                conn.send(audio_data)
                
            except WebSocketDisconnect:
                logging.info("WebSocket disconnected")
                break
            
            except Exception as e:
                logging.error(f"Error processing audio data: {e}")
                await websocket.send_text(json.dumps({"error": str(e)}))
                break
                
    except WebSocketDisconnect:
        logging.info("Disconnected")
    except Exception as e:
        logging.error(f"Error in WebSocket handler: {e}")

    finally:
        try:
            if 'conn' in locals():
                conn.finish()
            if 'queue_task' in locals():
                queue_task.cancel()
    
        except Exception as e:
            logging.error(f"Error in cleaning connection or finalizing events: {e}")